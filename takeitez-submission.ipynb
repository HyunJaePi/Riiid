{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TakeItEZ -- submission\n",
    "### HyunJae Pi, hyunpi@brandeis.edu, 12/10/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import riiideducation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import datatable as dt\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train = dt.fread('../input/riiid-test-answer-prediction/train.csv').to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train = train.astype({'row_id' : 'int32',\n",
    "                      'timestamp': 'int64',\n",
    "                      'user_id': 'int32',\n",
    "                      'content_id': 'int16',\n",
    "                      'content_type_id': 'int8',\n",
    "                      'task_container_id': 'int16',\n",
    "                      'user_answer': 'int8',\n",
    "                      'answered_correctly': 'int8',\n",
    "                      'prior_question_elapsed_time': 'float32',\n",
    "                      'prior_question_had_explanation': 'bool',\n",
    "                      })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['row_id', 'task_container_id', 'user_answer'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['prior_question_elapsed_time'].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# user dataframe\n",
    "user_df = train[train.answered_correctly != -1].groupby('user_id').agg({'answered_correctly': ['count', 'mean'], 'prior_question_had_explanation': ['mean']}).reset_index()\n",
    "user_df.columns = ['user_id', 'user_n_questions_answered', 'user_mean_accuracy','user_boolean_mean_prior_question_had_explanation'] \n",
    "\n",
    "user_df['user_n_questions_answered'] = np.log(user_df['user_n_questions_answered'])\n",
    "\n",
    "user_lect = train.groupby([\"user_id\", \"answered_correctly\"]).size().unstack()\n",
    "user_lect.columns = ['Lecture', 'Wrong', 'Right'] # -1, 0, 1\n",
    "user_lect['Lecture'] = user_lect['Lecture'].fillna(0)\n",
    "user_lect = user_lect.astype('Int64')\n",
    "user_lect['user_watched_lecture'] = np.where(user_lect.Lecture > 0, 1, 0)\n",
    "user_lect = user_lect.reset_index()\n",
    "user_lect = user_lect[['user_id', 'user_watched_lecture']]\n",
    "\n",
    "user_df = user_df.merge(user_lect, on = \"user_id\", how = \"left\")\n",
    "del user_lect\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log (user_n_questions_answered)\n",
    "n_answers = user_df.user_n_questions_answered\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "n_answers.plot.hist(bins=100)\n",
    "plt.xlabel('log(user_n_question_answered)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df.to_csv('user_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# content dataframe\n",
    "content_df = train[train.answered_correctly != -1].groupby('content_id').agg({'answered_correctly': ['count', 'mean']}).reset_index()\n",
    "content_df.columns = ['content_id', 'content_n_questions', 'content_mean_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_df.to_csv('content_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# questions.csv\n",
    "questions_df = pd.read_csv('../input/riiid-test-answer-prediction/questions.csv')\n",
    "questions_df['num_tags'] = questions_df['tags'].apply(lambda x:len(x.split()) if pd.notna(x) else 0)\n",
    "questions_df = questions_df[['question_id','part','num_tags']]\n",
    "questions_df.columns = ['content_id','part','num_tags'] # changed the column names to merge it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_df.to_csv('questions_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "user_df = dt.fread('../input/dataframes/user_df.csv').to_pandas()\n",
    "content_df = dt.fread('../input/dataframes/content_df.csv').to_pandas()\n",
    "questions_df = dt.fread('../input/dataframes/questions_df.csv').to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train = train.merge(user_df, on = \"user_id\", how = \"left\")\n",
    "del user_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train = train.astype({'user_n_questions_answered' : 'int8',  \n",
    "                      'user_mean_accuracy' : 'float32',\n",
    "                      'user_boolean_mean_prior_question_had_explanation' : 'float32',\n",
    "                      'user_watched_lecture' : 'bool',\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train = train.merge(content_df, on = \"content_id\", how = \"left\")\n",
    "del content_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['content_n_questions'].fillna(0, inplace = True)\n",
    "train['content_mean_accuracy'].fillna(0.5, inplace = True)\n",
    "train = train.astype({'content_n_questions' : 'int16',\n",
    "                      'content_mean_accuracy' : 'float32',    \n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train = train.merge(questions_df, on = \"content_id\", how = \"left\")\n",
    "del questions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['part'].fillna(0, inplace = True)\n",
    "train['num_tags'].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.astype({'part' : 'category',\n",
    "                      'num_tags' : 'int8',\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train = pd.read_pickle('../input/train-tmp/train_tmp.pkl.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user engagement\n",
    "one_month = 31536000000/12\n",
    "train['user_engagement'] = np.where(train.timestamp > one_month, False, True)\n",
    "train.drop('timestamp', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.part = train.part.astype('category') \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train.to_pickle('preprocessed_train_v06.pkl.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # smaller dataset (quarter)\n",
    "# train = train.sample(frac=1).reset_index(drop=True) # shuffle data\n",
    "# idx = int(len(train)/8)\n",
    "# train[0:idx].to_pickle('preprocessed_train_v07_small.pkl.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import FileLink\n",
    "# FileLink(r'./preprocessed_train_v07.pkl.gzip')\n",
    "# FileLink(r'./preprocessed_train_v07_small.pkl.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## light GMB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import riiideducation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import datatable as dt\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#train = pd.read_pickle('../input/preprocessed-train-v07/preprocessed_train_v07_small.pkl.gzip')\n",
    "train = pd.read_pickle('../input/preprocessed-train-v07/preprocessed_train_v07.pkl.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12/10a -- too avoid memory error, use only half the data -- failed: timeout error >9hr running\n",
    "# 12/10b -- 1/3 data\n",
    "\n",
    "idx = int(len(train)/3)\n",
    "#print(idx)\n",
    "train = train[0:idx]\n",
    "#print(len(train))\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# shuffle & remove lectures\n",
    "train = train.sample(frac=1).reset_index(drop=True) # shuffle data\n",
    "train = train[train.answered_correctly != -1] # remove lectures = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part was set to 'category' type and caused an error\n",
    "train['part'] = train['part'].astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['prior_question_elapsed_time',\n",
    "            'prior_question_had_explanation', \n",
    "            'user_log_n_questions_answered',\n",
    "            'user_mean_accuracy', \n",
    "            'user_boolean_mean_prior_question_had_explanation', \n",
    "            'user_watched_lecture', \n",
    "            'content_mean_accuracy',\n",
    "            'part',\n",
    "            'num_tags',\n",
    "            'user_engagement',\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[features]\n",
    "y = train['answered_correctly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# train vs. validation dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(X_train, y_train, categorical_feature = ['part', 'user_engagement', 'user_watched_lecture', 'prior_question_had_explanation'])\n",
    "lgb_test = lgb.Dataset(X_test, y_test, categorical_feature = ['part', 'user_engagement', 'user_watched_lecture', 'prior_question_had_explanation'])\n",
    "\n",
    "del X_train, y_train, X_test, y_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X, y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'objective': 'binary',\n",
    "          'metric': 'auc',\n",
    "          'seed': 123,\n",
    "          'learning_rate': 0.1, \n",
    "          'boosting_type': 'gbdt',\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "clf = lgb.train(\n",
    "    params, lgb_train,\n",
    "    valid_sets=[lgb_train, lgb_test],\n",
    "    verbose_eval=50,\n",
    "    num_boost_round=10000,\n",
    "    early_stopping_rounds=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = lgb.plot_importance(clf)\n",
    "ax.figure.savefig('feature_importance_v07b.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a trained model\n",
    "fname = 'saved_model_v07b.sav'\n",
    "pickle.dump(clf, open(fname, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load a saved model\n",
    "# fname = '../input/saved-models/saved_model.sav'\n",
    "# clf = pickle.load(open(fname, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the saved model & Submit a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import riiideducation\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import datatable as dt\n",
    "# import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# load a saved model\n",
    "# fname = '../input/saved-models/saved_model_v07b.sav'\n",
    "# clf = pickle.load(open(fname, 'rb'))\n",
    "\n",
    "user_df = dt.fread('../input/dataframes/user_df.csv').to_pandas()\n",
    "content_df = dt.fread('../input/dataframes/content_df.csv').to_pandas()\n",
    "questions_df = dt.fread('../input/dataframes/questions_df.csv').to_pandas()\n",
    "\n",
    "# load env\n",
    "env = riiideducation.make_env()\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['prior_question_elapsed_time',\n",
    "            'prior_question_had_explanation', \n",
    "            'user_log_n_questions_answered',\n",
    "            'user_mean_accuracy', \n",
    "            'user_boolean_mean_prior_question_had_explanation', \n",
    "            'user_watched_lecture', \n",
    "            'content_mean_accuracy',\n",
    "            'part',\n",
    "            'num_tags',\n",
    "            'user_engagement',\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_month = 31536000000/12\n",
    "val_content_mean_accuracy = np.mean(content_df.content_mean_accuracy)\n",
    "val_user_mean_accuracy = np.mean(user_df.user_mean_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (test_df, sample_prediction_df) in iter_test:\n",
    "    test_df = test_df.merge(user_df, on = \"user_id\", how = \"left\")\n",
    "    test_df = test_df.merge(content_df, on = \"content_id\", how = \"left\")\n",
    "    test_df = test_df.merge(questions_df, on = \"content_id\", how = \"left\")\n",
    "    \n",
    "    # rename\n",
    "    test_df.rename(columns = {'user_n_questions_answered' : 'user_log_n_questions_answered'}, inplace=True)\n",
    "    \n",
    "    # user_engagement\n",
    "    test_df['user_engagement'] = np.where(test_df.timestamp > one_month, False, True)\n",
    "  \n",
    "    # fill NaNs with numbers\n",
    "    test_df['prior_question_elapsed_time'].fillna(0, inplace = True)\n",
    "    test_df['content_mean_accuracy'].fillna(val_content_mean_accuracy, inplace = True)\n",
    "    test_df['user_watched_lecture'].fillna(False, inplace = True)\n",
    "    test_df['user_mean_accuracy'].fillna(val_user_mean_accuracy, inplace = True)\n",
    "    test_df['prior_question_had_explanation'].fillna(0, inplace = True) ### use 0 for <NA> instead of False\n",
    "    test_df['prior_question_had_explanation'] = test_df['prior_question_had_explanation'].astype('bool')\n",
    "    \n",
    "\n",
    "#    test_df['prior_question_had_explanation'] = label_enc.fit_transform(test_df['prior_question_had_explanation'])\n",
    "\n",
    "    test_df['answered_correctly'] =  clf.predict(test_df[features])\n",
    "    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
